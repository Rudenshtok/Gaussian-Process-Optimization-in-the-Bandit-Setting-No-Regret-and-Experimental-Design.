\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage[14pt]{extsizes}
\usepackage{fullpage}
\usepackage{graphicx}
\graphicspath{{pictures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage[hidelinks]{hyperref,xcolor}
\usepackage{setspace,amsmath}
\usepackage{indentfirst}

\begin{document}

\begin{titlepage}
		\begin{center}
			\vspace*{12em}
			\large{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design.}\\
			\vspace{3em}
			\normalsize{.....}\\
			\vspace{6em}
			\small{Varvara \textsc{Rudenko}}\\
			\vspace{5em}
			\small{17.08.2020}\\
			\thispagestyle{empty}
		\end{center}                             
	\end{titlepage}

\newpage
	\tableofcontents
\newpage

\section{Introduction}
Firstly, I'd like to tell you about the problem itself. The article under study describes a task related to reinforcement learning. 

(In reinforcement learning, there is an agent that interacts with the environment by taking actions. The environment gives a reward for these actions, and the agent continues to take them. The environment is usually formulated as a Markov decision-making process with a finite set of states. The probabilities of wins and state transitions in MDP are usually random values, but stationary within the problem.)

\paragraph{MDP}
At each time step, the process is in some state  \textit{s}, and the decision maker may choose any action \textit{a} that is available in state \textit{s}. The process responds at the next time step by randomly moving into a new state \textit{s'}, and giving the decision maker a corresponding reward \textit{${R_{a}(s,s')}$}. 

\begin{figure}[h!]
	\center{\includegraphics[width=15cm]{pic/1}}
	\caption{Example of a simple MDP with three states (green circles) and two actions (orange circles), with two rewards (orange arrows).}
\end{figure}

\newpage

\paragraph{Problem statement in reinforcement learning}
\textit{S} - set of environment states. Agent's game with the environment:
\begin{enumerate}
	\item initializing strategy ${\pi_{1}(a|s)}$ and the state of the environment \textit{${s_{1}}$}
	\item for all \textit{t = 1...T}:
	\begin{enumerate}
		\item the agent selects the action ${a_{t}\sim\pi_{t}(a|s_{t})}$
		\item the environment generates a reward ${r_{t+1}\sim p(r|a_{t},s_{t})}$ and a new state ${s_{t+1}\sim p(s|a_{t},s_{t})}$
		\item the agent adjusts the strategy ${\pi_{t+1}(a|s)}$
	\end{enumerate}
\end{enumerate}
This is a Markov decision process (MDP) if
\begin{equation}
	P(s_{t+1} = s', r_{t+1} = r|s_{t}, a_{t}, r_{t}, s_{t-1}, a_{t-1}, r_{t-1},..,s_{1}, a_{1})=P(s_{t+1} = s', r_{t+1} = r|s_{t}, a_{t})
\end{equation}
An MDP is called finit if ${|A|<\infty}$ and ${|S|<\infty}$


\paragraph{Multi-armed bandit problem}

\begin{figure}[h!]
	\center{\includegraphics[width=15cm]{pic/2}}
	\caption{Multi-armed bandit.}
\end{figure}

\textit{A} - the set of possible actions (handles of the machine),\\
\textit{${p_{a}(r)}$} - unknown reward distribution ${r\in R}$, ${\forall a\in A}$ \\
\textit{${\pi_{t}(a)}$} - agent's strategy at the moment ${t, \forall a\in A}$ \\
Agent's game with the environment:
\begin{enumerate}
	\item initializing the strategy ${\pi_{1}(a)}$
	\item for all ${t=1...T}$:
	\begin{enumerate}
		\item the agent selects an action (handle) ${a_{t}\sim\pi_{t}(a)}$
		\item the environment generates a reward ${r_{t}\sim p_{a_{t}}(r)}$
		\item the agent adjusts the strategy ${\pi_{t+1}(a)}$
	\end{enumerate}
\end{enumerate}
${Q_{t}(a)=\frac{\sum_{i=1}^{t}r_{i}[a_{i}=a]}{\sum_{i=1}^{t}[a_{i}=a]}\rightarrow max}$ - average reward in \textit{t} games\\
${Q^{*}(a)=\lim_{t\rightarrow\infty}Q_{t}(a)\rightarrow max}$ - the value of action \textit{a}.\\

 
 
\end{document}