\documentclass[10pt]{beamer}
\usepackage{natbib}
\usepackage{colortbl}
\usepackage{color}
\hypersetup{
  colorlinks   = true, 
  urlcolor     = blue, 
  linkcolor    = blue!50!black, 
  citecolor    = green!30!black
}

\setlength{\leftmargini}{5mm}
\setbeamertemplate{section in toc}[sections numbered]
\mode<presentation> {

\usetheme{boxes}
\definecolor{mygreen}{cmyk}{0.82,0.11,1,0.25}
\setbeamertemplate{blocks}[rounded][shadow=false]
\addtobeamertemplate{block begin}{\pgfsetfillopacity{0.8}}{\pgfsetfillopacity{1}}

\setbeamercolor*{block title example}{fg=black!90,
bg= blue!8}
\setbeamercolor*{block body example}{fg= black,
bg= blue!3}
\setbeamercolor{block title alerted}{use=structure,fg=red,bg=red!5}
\setbeamercolor{block body alerted}{parent=normal text,use=block title,bg=red!1.5}
\setbeamercolor*{block title}{fg=black,
bg= black!8}
\setbeamercolor*{block body}{fg= black,
bg= black!3}
\setbeamercolor*{title}{use=structure,bg=blue!20}
\setbeamertemplate{title page}[default][colsep=-4bp,rounded=true,shadow=true]
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{}
}

\usepackage{graphicx}
\graphicspath{{./img/}}

\usepackage{booktabs}
\usepackage{tkz-graph}
\usepackage{makecell}
\GraphInit[vstyle = Shade]
\tikzset{
  LabelStyle/.style = { rectangle, rounded corners, draw,
                        minimum width = 2em, fill = yellow!50,
                        text = red, font = \bfseries },
  VertexStyle/.append style = { inner sep=5pt,
                                font = \normalsize\bfseries},
  EdgeStyle/.append style = {->, bend left} }
\usetikzlibrary {positioning}
\definecolor {processblue}{cmyk}{0.96,0,0,0}
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\usepackage{mwe}
\usepackage{amsthm,amsmath,dsfont}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}

\usepackage{multibib}
\newcites{pageone}{References}
\newcites{pagetwo}{References}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{array,booktabs,tabularx}
\usepackage{xargs}
\input{def.tex}%definitions

\title[Title]{Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design}

\author{\large{Rudenko Varvara} }
\institute[MIPT]
{\footnotesize
\includegraphics[width=0.45\linewidth]{pic/11.png}
}
\date{April, 2021} 


\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Summary} 
    \tableofcontents 
\end{frame}
 

\section{Abstract}
\begin{frame}
    \frametitle{Abstract}
    \begin{itemize}
        \item Many applications require optimizing an unknown, noisy   function that is expensive to evaluate. We formalize this task as a  multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP).  
        \item We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization.   We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design.
    \end{itemize}
\end{frame}


\section{Problem Statement and Background}
\begin{frame}
\frametitle{Problem Statement and Background}
    \begin{itemize}
    \item UCB in bandit setting:
        \begin{equation}
            A_{t}=argmax\left [ Q_{t}(a)+\beta_{t}\sqrt{\frac{ln t}{N_{t}(a)}} \right ],
        \end{equation}
        where ${Q_{t}}$ - evaluation of the action value after selecting t rounds.
    \item D - unity of all sensor locations, f(x) is the temperature at x, and sensor  accuracy  is  quantified  by  the  noise  variance.
    \item In our case, there are too many hands - we complicate the algorithm
    \item Gaussian process - a stochastic process, such that every finite collection of those random variables has a multivariate normal distribution.
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=300]{pic/2.png}
        \caption{(a) Example of temperature data collected by a network of 46 sensors at Intel Research Berkeley.}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{itemize}
        \item Gaussian noise, the posterior over f is a GP distribution again, with mean ${\mu_{T}(x)}$, covariance ${k_{T}(x,x')}$ and variance ${{\sigma_{T}}^2(x)}$:
        \begin{equation}
            \mu_{T}(x)=k_{T}(x)^{T}(K_{T}+\sigma^2I)^{-1}y_{T}
        \end{equation}
        \begin{equation}
            k_{T}(x,x')=k(x,x')-k_{T}(x)^{T}(K_{T}+\sigma^2I)^{-1}k_{T}(x')
        \end{equation}
        \begin{equation}
            {\sigma_{T}}^2(x)=k_{T}(x,x)
        \end{equation}\\
            \begin{equation}
                x_{t}=argmax  _{x\in D} \,\, \sigma_{t-1}(x),
            \end{equation}
        \item Another idea is to pick points as:
            \begin{equation}
                x_{t}=argmax _{x\in D} \,\, \mu_{t-1}(x),
            \end{equation}
        \item A  combined strategy is to choose:
            \begin{equation}
                x_{t}=(argmax_{x\in D} \,\, \mu_{t-1}(x) +{\beta_{t}}^{\frac{1}{2}} \,\, \sigma_{t-1}(x))
            \end{equation}
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=300]{pic/3.png}
    \end{figure}
    where ${\beta_{t}}$ are appropriate constants.
    \begin{itemize}
        \item Bayesian update is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=300]{pic/2.png}
        \caption{ (b,c) Two iterations of the GP-UCB algorithm.  It samples points that are either uncertain (b) or have high posterior mean (c).}
    \end{figure}
\end{frame}

\section{Experiments}
\begin{frame}
\frametitle{Experiments}
\begin{itemize}
    \item We  compare GP-UCB with  heuristics  such  as  the Expected   Improvement   (EI)   and   Most   Probable Improvement  (MPI),  and  with  naive  methods  which choose  points  of  maximum  mean  or  variance  only, both on synthetic and real sensor network data.
    \item Expected improvement is defined as
        \begin{equation}
            EI(x)=Emax(f(x)-f(x^+),0),
        \end{equation}
    where ${f(x^+)}$ is the value of the best sample so far and ${x^+}$ is the location of that sample.
    \item Most   Probable Improvement:
        \begin{equation}
            PI(x)=P(f(x)\geq f(x^+))=F\left ( \frac{\mu(x)-f(x^+)}{\sigma(x)} \right )
        \end{equation}
    where ${F(.)}$ is the normal cumulative distribution function. 
\end{itemize}
\end{frame}

\begin{frame}
    \begin{figure}
        \includegraphics[width=300]{pic/5.png}
        \caption{Comparison of performance: GP-UCB and various heuristics on synthetic (a), and sensor network data (b, c).}
    \end{figure}
\end{frame}

\section{Conclusions}
\begin{frame}{Conclusions}
    \begin{itemize}
    \item We  analyze GP-UCB, an  intuitive  algorithm  for GP optimization, when the function is either sampled from a known GP, or has low RKHS norm.
    \item We bound  the cumulative regret for GP-UCB interms of  the  information gain due to sampling, establishing a novel connection  between experimental design and GP optimization.
    \item By bounding the information gain for popular classes of kernels,  we establish sublinear regret bounds for GP optimization for the first  time.
    \item We  evaluate GP-UCB on  sensor  network data, demonstrating that it compares favorably to existing algorithms for GP optimization.
    \end{itemize}\vspace{.2cm}
\end{frame}

\begin{frame}
    \begin{center}
    {\large Thank you!}
\end{center}
    
\scriptsize
    \bibliographystyle{plainnat}
    \bibliography{biblio.bib}
\end{frame}

\end{document}

